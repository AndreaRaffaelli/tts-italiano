# Enhanced TTS Training Pipeline

Una pipeline modulare e completa per il fine-tuning di modelli Text-to-Speech italiani usando SpeechT5.

## ğŸš€ Caratteristiche

- **Modulare**: Architettura ben organizzata con separazione delle responsabilitÃ 
- **Cache intelligente**: Sistema di cache avanzato per evitare rielaborazioni
- **GPU ottimizzato**: Rilevamento automatico GPU e ottimizzazione batch size
- **Preprocessing robusto**: Preprocessing avanzato per audio e testo italiano
- **CLI interattiva**: Interfaccia a riga di comando user-friendly
- **Monitoraggio**: Sistema di logging e monitoraggio integrato
- **Hugging Face ready**: Integrazione completa con Hugging Face Hub

## ğŸ“‹ Requisiti

- Python 3.8+
- CUDA-capable GPU (raccomandato, ma funziona anche su CPU)
- Almeno 16GB di RAM
- Spazio su disco: ~10GB per dataset e cache

## âš¡ Installazione Rapida

```bash
# Clona il repository
git clone https://github.com/yourusername/enhanced-tts-training.git
cd enhanced-tts-training

# Installa dipendenze
pip install -r requirements.txt

# Oppure installa come package
pip install -e .
```

## ğŸ”§ Installazione Dettagliata

### 1. Dipendenze PyTorch (con CUDA)
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 2. Dipendenze principali
```bash
pip install datasets==3.6.0
pip install soundfile speechbrain accelerate
pip install git+https://github.com/huggingface/transformers.git
```

### 3. Dipendenze opzionali
```bash
# Per il development
pip install -e .[dev]

# Per logging avanzato
pip install -e .[logging]

# Tutto
pip install -e .[all]
```

## ğŸš€ Utilizzo

### Modo Base
```bash
python main.py
```

### Con parametri personalizzati
```bash
python main.py \
  --output-dir "my_italian_tts" \
  --max-steps 5000 \
  --learning-rate 1e-5 \
  --batch-size 4 \
  --cache-dir "my_cache"
```

### Modo non-interattivo
```bash
python main.py --yes --max-steps 10000 --push-to-hub
```

### Parametri disponibili

| Parametro | Descrizione | Default |
|-----------|-------------|---------|
| `--output-dir` | Directory output del modello | `speecht5_italian_enhanced` |
| `--max-steps` | Numero massimo di step di training | `10000` |
| `--learning-rate` | Learning rate | `3e-6` |
| `--batch-size` | Override batch size automatico | Auto-detect |
| `--cache-dir` | Directory per i file di cache | `cache` |
| `--force-reprocess` | Forza rielaborazione dati cached | False |
| `--clear-cache` | Pulisce tutta la cache | False |
| `--push-to-hub` | Push su Hugging Face Hub | False |
| `-y, --yes` | ModalitÃ  non-interattiva | False |
| `--verbose` | Logging verboso | False |

## ğŸ“ Struttura del Progetto

```
enhanced_tts_training/
â”œâ”€â”€ main.py                    # Entry point principale
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ training_config.py     # Configurazioni di training
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py       # Preprocessing audio e testo
â”‚   â”œâ”€â”€ dataset_loader.py      # Caricamento e gestione dataset
â”‚   â””â”€â”€ data_collator.py       # Data collator per il training
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_setup.py         # Setup modelli e processori
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py             # Logic di training
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli_utils.py           # Utilities CLI
â”‚   â”œâ”€â”€ cache_utils.py         # Gestione cache
â”‚   â””â”€â”€ gpu_utils.py           # Utilities GPU
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ setup.py                   # Package setup
â””â”€â”€ README.md                  # Questa documentazione
```

## ğŸ”„ Pipeline di Training

La pipeline segue questi passi principali:

1. **Check Dependencies**: Verifica installazione dipendenze
2. **Model Loading**: Carica SpeechT5 e modello speaker embedding
3. **Dataset Loading**: Carica VoxPopuli e Common Voice italiani
4. **Text Preprocessing**: Normalizza testo italiano
5. **Quality Filtering**: Filtra campioni di bassa qualitÃ 
6. **Audio Processing**: Preprocessing audio e generazione embedding
7. **Training Setup**: Configura trainer e parametri
8. **Training**: Esegue il fine-tuning
9. **Saving**: Salva modello e processor

## ğŸ›ï¸ Configurazione Avanzata

### Configurazione da file
```python
from config.training_config import EnhancedTrainingConfig

# Crea configurazione personalizzata
config = EnhancedTrainingConfig(
    max_steps=15000,
    learning_rate=2e-6,
    warmup_steps=2000,
    eval_steps=250,
    save_steps=500
)

# Salva configurazione
config.save_to_file("my_config.json")
```

### Utilizzo programmatico
```python
from training.trainer import create_trainer
from utils.gpu_utils import check_gpu

# Crea trainer
has_gpu = check_gpu()
trainer = create_trainer(args, has_gpu)

# Esegui training
success = trainer.run_training_pipeline()
```

## ğŸ“Š Monitoraggio

### TensorBoard
```bash
# Avvia TensorBoard
tensorboard --logdir speecht5_italian_enhanced/logs

# Oppure se hai specificato una directory diversa
tensorboard --logdir your_output_dir/logs
```

### Logs personalizzati
Il sistema genera automaticamente:
- Statistiche dataset
- Metriche di training
- Uso memoria GPU
- Curve di loss

## ğŸ”§ Risoluzione Problemi

### Out of Memory (OOM)
```bash
# Riduci batch size
python main.py --batch-size 1

# Oppure usa gradient checkpointing (giÃ  abilitato di default)
```

### Cache corrotta
```bash
# Pulisci cache
python main.py --clear-cache

# Oppure forza rielaborazione
python main.py --force-reprocess
```

### Datasets non scaricabili
Se i dataset non si scaricano:
1. Verifica connessione internet
2. Controlla spazio disco
3. Prova a scaricare manualmente

## ğŸš€ Esempi Avanzati

### Training con configurazione personalizzata
```bash
python main.py \
  --output-dir "italian_tts_v2" \
  --max-steps 20000 \
  --learning-rate 1e-5 \
  --cache-dir "/tmp/tts_cache" \
  --push-to-hub \
  --yes
```

### Solo preprocessing (senza training)
```python
from data.dataset_loader import DatasetManager
from utils.cache_utils import get_cache_paths

cache_paths = get_cache_paths("my_cache")
manager = DatasetManager(cache_paths, processor, speaker_model)
dataset = manager.load_and_process_datasets()
```

## ğŸ¤ Contribuire

1. Fork il repository
2. Crea un branch per la feature (`git checkout -b feature/nuova-feature`)
3. Commit le modifiche (`git commit -am 'Aggiunge nuova feature'`)
4. Push al branch (`git push origin feature/nuova-feature`)
5. Crea una Pull Request

## ğŸ“„ Licenza

Questo progetto Ã¨ distribuito sotto licenza MIT. Vedi il file `LICENSE` per dettagli.

## ğŸ™ Ringraziamenti

- [Hugging Face](https://huggingface.co/) per Transformers e Datasets
- [SpeechBrain](https://speechbrain.github.io/) per i modelli audio
- [Microsoft](https://github.com/microsoft/SpeechT5) per SpeechT5
- Community italiana di ML per feedback e test

## ğŸ“ Supporto

- Issues: [GitHub Issues](https://github.com/yourusername/enhanced-tts-training/issues)
- Discussions: [GitHub Discussions](https://github.com/yourusername/enhanced-tts-training/discussions)
- Email: your.email@example.com

---

**Nota**: Questo Ã¨ un progetto in sviluppo attivo. Le API possono cambiare tra le versioni.
